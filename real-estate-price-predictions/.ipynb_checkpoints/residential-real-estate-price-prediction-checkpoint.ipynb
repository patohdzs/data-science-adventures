{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Residential real estate price prediction using data from Ames, Iowa**\n---\n"},{"metadata":{},"cell_type":"markdown","source":"## **Contents**\n\n1 - Introduction\n\n2 - Import dependencies\n\n3 - Data retrieval and exploratory analysis\n\n4 - Data cleaning and preprocessing\n\n5 - Modelling\n\n6 - Final predictions and submission"},{"metadata":{},"cell_type":"markdown","source":"## **1 - Introduction**\n\nIn this notebook, I will aim to model house prices using a variety of machine learning algorithms and housing data from Ames, Iowa. Furthermore, I will attempt to forecast house prices for out-of-sample data and the resultant predictions will then be submmited to the Housing Prices Competition for Kaggle Learn Users. \n"},{"metadata":{},"cell_type":"markdown","source":"## **2 - Importing dependencies**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Importing libraries and modules\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import ElasticNet\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\nfrom warnings import simplefilter\nfrom sklearn.exceptions import ConvergenceWarning\nsimplefilter(\"ignore\", category=ConvergenceWarning)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **3 -  Data retrieval and exploratory analysis**\n\n\n3.1 - Data Retrieval\n\n3.2 - General Overview\n\n3.3 - Distribution of target variable\n\n3.4 - Distribution of features (numerical and non-numerical)\n\n\n3.5 - Bivariate analysis of numerical features against target variable\n\n3.6 - Correlation between features\n\n3.7 - Overview of missing values\n"},{"metadata":{},"cell_type":"markdown","source":"### 3.1 - Data retrieval"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading training data into Pandas DataFrame object\ndata = pd.read_csv('../input/home-data-for-ml-course/train.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2 - General overview"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Examining the first 10 dataset rows\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Examining the last five datset rows\ndata.tail(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Deleting 'Id' column from dataset\ndata = data.drop('Id', axis=1)\n\n# General overview of the dataset\ndata.info()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n#### **NOTES FROM (3.2):**\n\n* 80 columns\n* 1460 rows\n* First 79 columns correspond to features\n* The 80th column corresponds to the target variable `SalePrice`\n* 36 numerical features\n* 43 non-numerical features\n\n#### **OBSERVATIONS:**\n* Some numerical features could be categorical, thus further classification is required\n* Numerical features could be classified as 'object' due to erroneous entries\n"},{"metadata":{},"cell_type":"markdown","source":"### 3.3 - Distribution of target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Separating data for target variable\ntarget = data['SalePrice']\n\n# Summary statistics for target variable\ntarget.describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Plotting distribution of values for target variable\nplt.figure(figsize=(8,4))\nsns.distplot(target)\nplt.title('Distribution of SalePrice', size=16)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Plotting distribution of of log-transformed target variable\nlog_target = np.log(target)\nplt.figure(figsize=(8,4))\nsns.distplot(log_target)\nplt.title('Distribution of ln(SalePrice)', size=16)\nplt.show()\n\n# Calculating skewness of target variable data\nprint('SalePrice skewness: '+ str(target.skew()))\nprint('')\n\n# Calculating skewness of log-transformed target variable data\nprint('ln(SalePrice) skewness: '+ str(log_target.skew()))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### **NOTES FROM (3.3):**\n\n* No missing values for `SalePrice`\n* Distribution of house prices is significantly skewed\n\n#### **OBSERVATIONS:**\n* `SalePrice` data should be log-transformed to reduce skew (benefits regression models)\n* After fitting a model with log-tranformed `SalePrice` predictions should be inversely transformed\n"},{"metadata":{},"cell_type":"markdown","source":"### 3.4 - Distribution of features (numerical and non-numerical)"},{"metadata":{},"cell_type":"markdown","source":"\nIn Section 3.2 features were classified as either numerical or non-numerical, but it was noted that some numerical features might be categorical and some non-numerical features might be wrongly classified. The aim of this section is to further classify features into the [following categories](https://towardsdatascience.com/data-types-in-statistics-347e152e8bee) based on their data types, which will be more useful in terms of data analysis, preprocessing, and modelling:\n\n- Categorical (nominal)\n- Categorical (ordinal)\n- Discrete (interval)\n- Discrete (ratio)\n- Continuous (interval)\n- Continuous (ratio)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Separating data for numerical features\nnum_feats = data.select_dtypes(include='number').drop(['SalePrice'], axis=1).copy()\nnum_feats.describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Plotting distribution of data for numerical features\nfig = plt.figure(figsize=(16,22))\n\nfor i in range(len(num_feats.columns)):\n    \n    fig.add_subplot(9,4,i+1)\n    sns.distplot(num_feats.iloc[:,i].dropna(), hist=False, kde_kws={'bw':0.1})\n    \nplt.tight_layout()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Separating data for non-numerical features\nnot_num_feats = data.select_dtypes(exclude='number').copy()\nnot_num_feats.describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Plotting distribution of data for non-numerical features\nfig = plt.figure(figsize=(16,30))\n\nfor i in range(len(not_num_feats.columns)):\n    fig.add_subplot(11,4,i+1)\n    sns.countplot(x=not_num_feats.iloc[:,i].dropna())\n    plt.xticks(rotation=90)\n    plt.ylabel('Frequency')\n    \nplt.tight_layout()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nAfter analysing the distribution of values for all features, and with the aid of the data description file, we classified the features as follows:\n\n#### **FEATURE CLASSIFICATION:**\n\n\n* **Categorical (nominal):**\n> * other non_num feats!!!!!!!\n> * `MSSubClass`\n\n* **Categorical (ordinal):**\n> * `OverallQual`\n> * `OverallCond`\n> * `ExterQual`\n> * `ExterCond`\n> * `BsmtQual`\n> * `BsmtCond`\n> * `BsmtExposure`\n> * `BsmtFinType1`\n> * `BsmtFinType2`\n> * `HeatingQC`\n> * `KitchenQual`\n> * `Functional`\n> * `FireplaceQu`\n> * `GarageQual`\n> * `GarageCond`\n> * `GarageFinish`\n> * `PoolQC`\n> * `Fence`\n\n* **Discrete (ratio):**\n> * `YearBuilt`\n> * `YearRemodAdd`\n> * `BsmtFullBath`\n> * `BsmtHalfBath`\n> * `FullBath`\n> * `HalfBath`\n> * `BedroomAbvGr`\n> * `KitchenAbvGr`\n> * `TotRmsAbvGrd`\n> * `Fireplaces`\n> * `GarageYrBlt`\n> * `GarageCars`\n> * `GarageArea`\n> * `MoSold`\n> * `YrSold`\n\n* **Continuous (ratio):**\n> * `LotFrontage`\n> * `LotArea`\n> * `MasVnrArea`\n> * `BsmtFinSF1`\n> * `BsmtFinSF2`\n> * `BsmtUnfSF`\n> * `TotalBsmtSF`\n> * `1stFlrSF`\n> * `2ndFlrSF`\n> * `LowQualFinSF`\n> * `GrLivArea`\n> * `WoodDeckSF`\n> * `OpenPorchSF`\n> * `EnclosedPorch`\n> * `3SsnPorch`\n> * `ScreenPorch`\n> * `PoolArea`\n> * `MiscVal`\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nnominal_feats=['MSSubClass',]\nordinal_feats=['OverallQual','OverallCond','ExterQual','ExterCond','BsmtQual',\n               'BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','HeatingQC',\n               'KitchenQual','Functional','FireplaceQu','GarageQual','GarageCond',\n               'GarageFinish','PoolQC','Fence']\n\ndisc_feats=['YearBuilt','YearRemodAdd','BsmtFullBath','BsmtHalfBath',\n            'FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr','TotRmsAbvGrd',\n            'Fireplaces','GarageYrBlt','GarageCars','GarageArea','MoSold','YrSold']\n\ncont_feats=[]\n\n#FINISH!!!!!","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.5 - Bivariate analysis of numerical features against target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Plotting scaterrplots of numerical features versus the target variable 'SalePrice'\nfig = plt.figure(figsize=(16,22))\nfor i in range(len(num_feats.columns)):\n    fig.add_subplot(9, 4, i+1)\n    sns.scatterplot(num_feats.iloc[:, i],data['SalePrice'])\nplt.tight_layout()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.6 - Correlation between numerical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Calculating correlation matrix of numerical features\ncorrelations = num_feats.corr()\n\nf, ax = plt.subplots(figsize=(10,8))\nplt.title('Correlation of numerical attributes', size=16)\nsns.heatmap(correlations)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Finding correlation matrix for highly correlated features\nhigh_corr = correlations[(correlations>abs(0.7)) & (correlations!=1)]\nhigh_corr=high_corr.dropna(how='all')\nhigh_corr=high_corr.dropna(axis=1,how='all')\nhigh_corr\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### **Highly correlated features:**\n\n- `YearBuilt` and `GarageYrBlt`\n- `TotalBsmtSF` and `1stFlrSF`\n- `GrLivArea` and `TotRmsAbvGrd`\n- `GarageCars` and `GarageArea`"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Examining correlation of numerical features with \ntarget_corr = num_feats.corrwith(data['SalePrice'])\ntarget_corr = target_corr.sort_values(ascending=False)\n\nprint('Correlation of numerical features with target variable:')\nprint('')\n\ntarget_corr\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### **NOTES FROM (3.4):**\n\n* The following features are highly correlated:\n    * MoSold\n    * YrSold\n    * GarageCars\n* The following variables should be deleted:\n    * MoSold\n    * YrSold\n    * GarageCars\n"},{"metadata":{},"cell_type":"markdown","source":"### 3.7 - Overview of missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Plotting missing value counts for features in training data\n\nmissing = data.isnull().sum()\nmissing = missing[missing>0]\n#missing.sort_values(inplace=True)\nmissing = missing.to_frame()\nmissing.reset_index(inplace=True)\nmissing.columns=['Features', 'Number of missing values']\n\nplt.figure(figsize=(8, 5))\nsns.barplot(x='Features', y='Number of missing values', data=missing)\nplt.xticks(rotation=60)\nplt.title('Missing values in dataset features', size=16)\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking dataset missing values as percentage of total entries\nnull = pd.DataFrame(data={'%Missing Values': data.isnull().mean()[data.isnull().mean() > 0]})\nnull = round(null* 100, 2)\nnull = null.sort_values(by= '%Missing Values', ascending=False)\n\nnull.index.name='Feature'\nnull\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **4 - Data cleaning and preprocessing**\n\n4.1 - Outliers\n\n4.2 - Highly correlated features \n\n4.3 - Missing values\n\n4.4 - Categorical variables\n"},{"metadata":{},"cell_type":"markdown","source":"### 4.1 - Outliers"},{"metadata":{},"cell_type":"markdown","source":"From section 3.4, we identified potential outlier data points for the following features:\n- `LotFrontage`\n- `LotArea`\n- `MasVnrArea`\n- `BsmtFinSF1`\n- `TotalBsmtSF`\n- `1stFlrSF`\n- `LowQualFinSF`\n- `GrlLivArea`\n- `EnclosedPorch`\n- `MiscVal`\n\nBy making a closer examination of these features' scatterplots against `SalePrice` we can confirm whether they are outliers and delete their entries from the dataset:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Plotting enlarged scatterplots of numerical features with possible outliers versus 'SalePrice', along with fitted regresssion lines\nfigure, ((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8), (ax9, ax10)) = plt.subplots(nrows=5, ncols=2)\nfigure.set_size_inches(16,28)\n_ = sns.regplot(data['LotFrontage'], data['SalePrice'], ax=ax1)\n_ = sns.regplot(data['LotArea'], data['SalePrice'], ax=ax2)\n_ = sns.regplot(data['MasVnrArea'], data['SalePrice'], ax=ax3)\n_ = sns.regplot(data['BsmtFinSF1'], data['SalePrice'], ax=ax4)\n_ = sns.regplot(data['TotalBsmtSF'], data['SalePrice'], ax=ax5)\n_ = sns.regplot(data['1stFlrSF'], data['SalePrice'], ax=ax6)\n_ = sns.regplot(data['LowQualFinSF'], data['SalePrice'], ax=ax7)\n_ = sns.regplot(data['GrLivArea'], data['SalePrice'], ax=ax8)\n_ = sns.regplot(data['EnclosedPorch'], data['SalePrice'], ax=ax9)\n_ = sns.regplot(data['MiscVal'], data['SalePrice'], ax=ax10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Deleting outliers from dataset\ndata = data.drop(data[data['LotFrontage']>200].index)\ndata = data.drop(data[data['LotArea']>100000].index)\ndata = data.drop(data[data['MasVnrArea']>1200].index)\ndata = data.drop(data[data['BsmtFinSF1']>4000].index)\ndata = data.drop(data[data['TotalBsmtSF']>4000].index)\ndata = data.drop(data[data['1stFlrSF']>4000].index)\ndata = data.drop(data[(data['LowQualFinSF']>550) & (data['SalePrice']>400000)].index)\ndata = data.drop(data[data['GrLivArea']>4000].index)\ndata = data.drop(data[data['EnclosedPorch']>500].index)\ndata = data.drop(data[data['MiscVal']>5000].index)\n\n# Printing dataset summary\nprint('')\ndata.info(verbose= False)\nprint('')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2 - Missing Values"},{"metadata":{},"cell_type":"markdown","source":"\nFrom section 3.2 and the vizualizations in section 3.6, two things are made evident:\n\n1. There are many more features with missing data in the test dataset than in the training dataset\n2. The same handful of features (`PoolQC`, `MiscFeature`, `Alley`, `Fence`, and `FireplaceQu`) are missing data for the majority of entries in both the training and test dataset\n\nIn order to asses this in greater detail, we compute the table below, which shows the number of missing values reported for each feature as a percentage of the total number of entries:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Checking training and testing datasets missing value percentage\nnull = pd.DataFrame(data={'%Missing Values (training)': data.isnull().mean()[data.isnull().mean() > 0]})\nnull = round(null* 100, 2)\nnull = null.sort_values(by= '%Missing Values (training)', ascending=False)\n\nnull.index.name='Feature'\nnull\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We decide to deal with missing values as follows:\n- The following features will be deleted from both datasets: `PoolQC`, `MiscFeature`, `Alley`, `Fence`, `FireplaceQu`\n- For numeric features with a continuous sample space, missing values will be replaced by the feature mean\n- For numeric features with a discrete sample space, missing values will be replaced by the rounded feature mean\n- For categorical features, missing values will be replaced by the feature mode\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Deleting PoolQC, MiscFeature, Alley, Fence, and FireplaceQu from training and test datasets\ndata = data.drop(columns= ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu'])\ntest_data = test_data.drop(columns= ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu'])\n\n# Separating numerical feature names and categorical feature names\nnum_cont_feats = list(data.select_dtypes(include='float').columns)\nnum_dis_feats = list(data.select_dtypes(include='int').drop(['SalePrice'], axis=1).columns)\ncat_feats = list(data.select_dtypes(exclude='number').columns)\n\n# Filling in missing values\nfor feat in num_cont_feats:\n    data[feat] = data[feat].fillna(data[feat].mean())\n    test_data[feat] = test_data[feat].fillna(test_data[feat].mean())\n\nfor feat in num_dis_feats:\n    data[feat] = data[feat].fillna(round(data[feat].mean()))\n    test_data[feat] = test_data[feat].fillna(round(test_data[feat].mean()))\n\n\nfor feat in cat_feats:\n    data[feat] = data[feat].fillna(data[feat].mode()[0])\n    test_data[feat] = test_data[feat].fillna(test_data[feat].mode()[0])\n\n# Printing dataset summary\nprint('')\nprint('Training dataset:')\ndata.info(verbose=False)\nprint('')\nprint('Test dataset:')\ntest_data.info(verbose=False)\nprint('')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4 - Categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Getting categorical feature column names\ncat_feats = list(data.select_dtypes(exclude='number').columns)\n\n# Transforming categorical features into dummy features\ndata = pd.get_dummies(data, columns=cat_feats, prefix= cat_feats)\ntest_data = pd.get_dummies(test_data, columns=cat_feats, prefix= cat_feats)\n\n# Printing dataset summary\nprint('')\nprint('Training dataset:')\ndata.info(verbose=False)\nprint('')\nprint('Test dataset:')\ntest_data.info(verbose=False)\nprint('')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nAfter inspecting the summary of both datasets, it is evident that the training datset now contains data on more features than the test dataset. This occurs due to the dummy encoding process in the previous cell, as it is likely that some categorical features in the training dataset contain values that aren't reported for any of the entries in the test dataset, resulting in additional dummy features being created. In order to prevent these features from being fitted into the model, we delete them from the training dataset:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Deleting training dataset features that don't appear in test dataset\ntest_feats = list(test_data.columns) #.append('SalePrice')\ntest_feats.append('SalePrice')\ntest_feats.remove('Id')\n\ndata = data[test_feats]\n\n# Printing dataset summary\nprint('')\nprint('Training dataset:')\ndata.info(verbose=False)\nprint('')\nprint('Test dataset:')\ntest_data.info(verbose=False)\nprint('')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **5 - Modelling**"},{"metadata":{},"cell_type":"markdown","source":"### 5.1 - Evaluation metrics"},{"metadata":{},"cell_type":"markdown","source":"\nAs specified in the competition details, the metric used to evaluate prediction accuracy is the Root-Mean-Squared-Error between the logarithm of actual sale prices and the logarithm of predicted sale prices (RMSLE). Nevertheless, the score displayed on the competition leaderboard actually corresponds to the Mean-Absolute-Error (MAE) of predictions. Therefore, we define the following function below so that we can assess the accuracy of our models based on these two metrics.\n"},{"metadata":{},"cell_type":"markdown","source":"### 5.2 - Defining feature matrices and target vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Defining training feature matrix and target vector\nX_train = data.drop(columns='SalePrice')\ny_train= data['SalePrice']\n\n# Defining test feature matrix\nX_test = test_data\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2 - Initial model comparison"},{"metadata":{},"cell_type":"markdown","source":"In this section, we compare the performance of different prediction models prior to hyperparameter tuning. Since the target variable `SalePrice` is continuous, making its prediction a regression problem, the following models will be compared:\n\n* Linear regression\n* Ridge regression\n* Lasso regression\n* Elastic Net regression\n* KNN regressor\n* Decision Tree regressor\n* Random Forest regressor\n\nFor two of these models (Elastic Net and Lasso) feature data will be standardized as this is known to significantly improve performance. For the KNN model, feature data will be normalized for the same reason. Finally, the maximum number of itterations for the Lasso model has been increased to 30,000 to facilitate convergence.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Instantiating cross-validation score arrays:\n# Feature data for Elastic Net and Lasso models is standardized\n# Feature data for KNN is normalized\n\nmodels = {'Linear regression': Pipeline([('model', LinearRegression())]),\n          'Ridge': Pipeline([('scaler', StandardScaler()),('model', Ridge())]),\n          'Lasso': Pipeline([('scaler', StandardScaler()),('model', Lasso(max_iter=5000))]),\n          'Elastic Net': Pipeline([('scaler', StandardScaler()),('model', ElasticNet(max_iter=5000))]), \n          'KNN': Pipeline([('scaler', Normalizer()),('model', KNeighborsRegressor())]),\n          'Decision Tree': Pipeline([('model', DecisionTreeRegressor(random_state=0))]),\n          'Random Forest': Pipeline([('model', RandomForestRegressor(random_state=0))])}\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nNext, we utilize 10-fold cross-validation on the training dataset to compute the average Mean Absolute Error across folds:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# 10-fold cross-validation of predictive models using default hyperparameters\n\ncv_mae_scores = []\ncv_training_times = []\n\nfor key in models:\n    print('Running '+ key+ ' model...')\n    \n    %time cv = cross_validate(models[key], X_train, y_train, scoring='neg_mean_absolute_error',cv=10)\n    cv_mae_scores.append(cv['test_score'].mean())\n    cv_training_times.append(cv['fit_time'].mean())\n    \n    print('Finished running.')\n    print('')\n\nscores = pd.DataFrame(data={'Model': list(models.keys()),\n                            'Average MAE ($USD)': cv_mae_scores,\n                            'Average Training Time (seconds)': cv_training_times})\n\nscores['Average MAE ($USD)'] = round(scores['Average MAE ($USD)']*(-1), 3)\nscores = scores.set_index('Model').sort_values(by='Average MAE ($USD)', ascending=True)\nscores\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the console output above, two things can be concluded:\n\n1. KNN and Decision trees performed significantly worse than all other models.\n2. Random Forest and Lasso regression were decisively slower.\n\nNevertheless, since none of these models are exceedingly computationally expensive, we will carry on comparing all of them as their performance could change significantly after hyperparameters are tuned."},{"metadata":{},"cell_type":"markdown","source":"### 5.3 - Hyperparameter Tuning"},{"metadata":{},"cell_type":"markdown","source":"\nIn order to carry out hyperparameter tuning, we first specify the parameters that will be tuned in each model as well as the range of values that they may take:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Hyperparameters for Ridge regression\nridge_params={'model__alpha': np.linspace(0.5,20,40),\n              'scaler__with_mean':[True,False],\n              'scaler__with_std':[True,False]}\n\n# Hyperparameters for Lasso regression\nlasso_params={'model__alpha': np.linspace(0.5,20,40),\n              'scaler__with_mean':[True,False],\n              'scaler__with_std':[True,False]}\n\n# Hyperparameters for Elastic Net regression\ne_net_params={'model__alpha': np.linspace(1,20,20),\n              'model__l1_ratio': np.linspace(0,1,21),\n              'scaler__with_mean':[True,False],\n              'scaler__with_std':[True,False]}\n\n# Hyperparameters for KNN regressor\nknn_params={'model__n_neighbors': np.arange(1,50,1)}\n\n# Hyperparameters for Decision Tree regressor\ndt_params={}\n\n# Hyperparameters for Random Forest regressor\nrf_params={}\n\nmodel_param_grids = {'Ridge': ridge_params,\n                     'Lasso': lasso_params,\n                     'Elastic Net': e_net_params,\n                     'KNN': knn_params}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nWe now utilize grid search 5-fold cross-validation to iterate over the hyperparameter value ranges and find the optimal combination:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncv_mae_scores = []\n\nfor key in models:\n    \n    print('Running '+ key+ ' model...')\n    \n    if key in ['Ridge','Lasso', 'KNN']:\n        grid_search_cv = GridSearchCV(models[key], model_param_grids[key], scoring='neg_mean_absolute_error')\n        %time grid_search_cv.fit(X_train, y_train)\n        cv_mae_scores.append(grid_search_cv.best_score_)\n        print(grid_search_cv.best_params_)\n        \n    elif key in ['Elastic Net']:\n        rand_search_cv = RandomizedSearchCV(models[key], \n                                            model_param_grids[key],\n                                            scoring='neg_mean_absolute_error', \n                                            random_state=0)\n        \n        %time rand_search_cv.fit(X_train, y_train)\n        cv_mae_scores.append(rand_search_cv.best_score_)\n        print(rand_search_cv.best_params_)\n    \n    else:\n        %time cv = cross_validate(models[key], X_train, y_train, scoring='neg_mean_absolute_error',cv=10)\n        cv_mae_scores.append(cv['test_score'].mean())\n    \n    print('Finished running.')\n    print('')\n\nscores = pd.DataFrame(data={'Model': list(models.keys()),\n                            'Average MAE ($USD)': cv_mae_scores})\n\nscores['Average MAE ($USD)'] = round(scores['Average MAE ($USD)']*(-1), 3)\nscores = scores.set_index('Model').sort_values(by='Average MAE ($USD)', ascending=True)\nscores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.5 - Final Model"},{"metadata":{},"cell_type":"markdown","source":"The final Random Forrest model is instantiated with the optimal hyperparameter arguments derived in section 5.4 and trained using the features selected in section 5.3."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separating target variable vector y\ny = data['SalePrice']\n\n# Creating feature matrix X\nX = data[selected_features]\n\n# To improve accuracy, create a new Random Forest model which you will train on all training data\nrf_final_model = RandomForestRegressor(random_state=0, n_estimators=180, max_depth=16 )\n\n# fit rf_model_on_full_data on all data from the training data\nrf_final_model.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **6 - Final predictions and submission**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Creating feature matrix for test data\ntest_X = test_data[selected_features]\n\n# Predicting SalePrice for test data \ntest_preds = rf_final_model.predict(test_X )\n\n# Creating submission file\noutput = pd.DataFrame({'Id': test_data.Id,'SalePrice': test_preds})\noutput.to_csv('submission1.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}